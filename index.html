<!doctype html><html lang="en"><head><meta charset="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Global PDF Processor</title><script src="https://cdn.jsdelivr.net/pyodide/v0.24.1/full/pyodide.js"></script><script type="text/javascript">async function initGlobalPDFProcessor(){if(!window.pdfProcessor.loading&&!window.pdfProcessor.ready)try{console.log("üîÑ Initializing Global PDF Processor..."),window.pdfProcessor.loading=!0,window.pdfProcessor.error=null,console.log("üì¶ Loading Pyodide..."),window.pdfProcessor.pyodide=await loadPyodide({indexURL:"https://cdn.jsdelivr.net/pyodide/v0.24.1/full/"}),console.log("üìö Installing Python packages..."),await window.pdfProcessor.pyodide.loadPackage("micropip"),console.log("üì¶ Installing PDF processing packages...");let e=!1;try{await window.pdfProcessor.pyodide.runPython('\nimport micropip\nprint("üì¶ Attempting to install PyPDF2...")\n            ');const n=["PyPDF2","PyPDF2==3.0.1","PyPDF2==2.12.1"];for(const t of n)try{console.log(`üì¶ Trying ${t}...`),await window.pdfProcessor.pyodide.runPythonAsync(`\nawait micropip.install(['${t}'])\nprint("‚úÖ ${t} installed successfully")\n                `),e=!0;break}catch(e){console.warn(`‚ö†Ô∏è Failed to install ${t}:`,e.message);continue}e||(console.warn("‚ö†Ô∏è Could not install PyPDF2, will use text-only processing"),await window.pdfProcessor.pyodide.runPython("\nimport json\nimport re\nimport string\nfrom collections import Counter\n\n# Minimal text processor without PDF support\nclass SimpleTextProcessor:\n    def __init__(self):\n        self.stop_words = {\n            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', \n            'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those'\n        }.union(set(string.punctuation))\n    \n    def simple_summarize(self, text, max_sentences=3):\n        if not text or len(text) < 200:\n            return text\n        \n        sentences = re.split(r'[.!?]+', text)\n        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]\n        \n        if len(sentences) <= max_sentences:\n            return '. '.join(sentences) + '.'\n        \n        # Simple scoring\n        scored_sentences = []\n        for i, sentence in enumerate(sentences):\n            score = len(sentence.split())\n            if i == 0:\n                score *= 1.5\n            scored_sentences.append((score, sentence, i))\n        \n        top_sentences = sorted(scored_sentences, key=lambda x: x[0], reverse=True)[:max_sentences]\n        top_sentences = sorted(top_sentences, key=lambda x: x[2])\n        \n        return '. '.join([sent[1] for sent in top_sentences]) + '.'\n    \n    def extract_keywords(self, text, max_keywords=8):\n        # Simple word extraction\n        words = text.lower().split()\n        filtered_words = [w.strip('.,!?;:\"()[]') for w in words if len(w) > 4]\n        filtered_words = [w for w in filtered_words if w not in self.stop_words and w.isalpha()]\n        word_freq = Counter(filtered_words)\n        \n        keyword_scores = []\n        for word, freq in word_freq.items():\n            score = freq\n            if len(word) > 6:\n                score *= 1.2\n            keyword_scores.append({'word': word, 'score': score, 'frequency': freq})\n        \n        return sorted(keyword_scores, key=lambda x: x['score'], reverse=True)[:max_keywords]\n\n# Create simple processor\nsimple_processor = SimpleTextProcessor()\n\ndef process_pdf_global(pdf_bytes):\n    return json.dumps({\n        'success': False,\n        'error': 'PDF processing not available - PyPDF2 installation failed',\n        'text': '',\n        'summary': '',\n        'keywords': [],\n        'processing_method': 'unavailable'\n    })\n\ndef summarize_text_global(text, max_sentences=3):\n    return simple_processor.simple_summarize(text, max_sentences)\n\ndef extract_keywords_global(text, max_keywords=8):\n    keywords = simple_processor.extract_keywords(text, max_keywords)\n    return json.dumps(keywords)\n\nimport js\njs.processPDFGlobal = process_pdf_global\njs.summarizeTextGlobal = summarize_text_global\njs.extractKeywordsGlobal = extract_keywords_global\n\nprint(\"‚ö° Text processing ready (PDF support unavailable)\")\n              "))}catch(e){console.warn("‚ö†Ô∏è Package installation failed, setting up minimal processor",e)}if(console.log("üõ†Ô∏è Setting up PDF processing functions..."),e){const e="\nimport js\nimport json\nimport re\nimport io\nimport string\nfrom typing import Dict, Any, List\nimport PyPDF2\nfrom collections import Counter\n\nclass GlobalPDFExtractor:\n    def __init__(self):\n        self.stop_words = self._get_basic_stop_words()\n        \n    def _get_basic_stop_words(self):\n        \"\"\"Basic English stop words for keyword extraction\"\"\"\n        return {\n            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', \n            'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those',\n            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', \n            'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their', 'from', \n            'up', 'out', 'down', 'off', 'over', 'under', 'again', 'further', \n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', \n            'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', \n            'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', \n            'than', 'too', 'very', 'just', 'now', 'also', 'time', 'way'\n        }.union(set(string.punctuation))\n    \n    def extract_text_from_pdf_bytes(self, pdf_bytes):\n        \"\"\"Extract clean text from PDF bytes using PyPDF2\"\"\"\n        try:\n            # Ensure we have Python bytes, not JavaScript typed array or memoryview\n            if hasattr(pdf_bytes, 'to_py'):\n                pdf_bytes = pdf_bytes.to_py()\n            \n            # Convert memoryview to bytes if needed\n            if isinstance(pdf_bytes, memoryview):\n                pdf_bytes = bytes(pdf_bytes)\n                print(f\"üìã Converted memoryview to bytes in extractor\")\n            \n            # Create BytesIO object from bytes\n            pdf_file = io.BytesIO(pdf_bytes)\n            pdf_reader = PyPDF2.PdfReader(pdf_file)\n            \n            extracted_text = \"\"\n            page_count = len(pdf_reader.pages)\n            \n            print(f\"üìÑ Processing PDF with {page_count} pages...\")\n            \n            for i, page in enumerate(pdf_reader.pages):\n                try:\n                    page_text = page.extract_text()\n                    if page_text:\n                        extracted_text += page_text + chr(10)  # Use chr(10) for newline\n                        print(f\"‚úÖ Processed page {i+1}/{page_count}\")\n                except Exception as e:\n                    print(f\"‚ö†Ô∏è Warning: Could not extract text from page {i+1}: {e}\")\n                    continue\n            \n            if not extracted_text.strip():\n                return {\n                    'success': False,\n                    'error': 'No readable text found in PDF',\n                    'text': '',\n                    'char_count': 0,\n                    'word_count': 0\n                }\n            \n            # Clean the extracted text\n            cleaned_text = self._clean_text(extracted_text)\n            word_count = len(cleaned_text.split())\n            char_count = len(cleaned_text)\n            \n            print(f\"üìù Extracted {char_count} characters, {word_count} words\")\n            \n            return {\n                'success': True,\n                'text': cleaned_text,\n                'char_count': char_count,\n                'word_count': word_count,\n                'page_count': page_count\n            }\n            \n        except Exception as e:\n            error_msg = f\"Error extracting PDF: {str(e)}\"\n            print(f\"‚ùå {error_msg}\")\n            return {\n                'success': False,\n                'error': error_msg,\n                'text': '',\n                'char_count': 0,\n                'word_count': 0\n            }\n    \n    def _clean_text(self, text):\n        \"\"\"Clean and normalize extracted text\"\"\"\n        # Remove excessive whitespace and normalize\n        cleaned = re.sub(r'\\s+', ' ', text)\n        cleaned = cleaned.strip()\n        \n        # Remove non-printable characters but keep basic punctuation\n        # Use simple ASCII range\n        cleaned = ''.join(c for c in cleaned if ord(c) >= 32 and ord(c) <= 126 or c == '\\n')\n        \n        # Normalize multiple spaces\n        cleaned = re.sub(r' +', ' ', cleaned)\n        \n        return cleaned\n    \n    def extract_keywords(self, text, max_keywords=10):\n        \"\"\"Extract meaningful keywords from text\"\"\"\n        try:\n            # Use simple word splitting instead of complex regex\n            words = text.lower().split()\n            # Remove punctuation from words\n            cleaned_words = []\n            for word in words:\n                clean_word = ''.join(c for c in word if c.isalpha())\n                if len(clean_word) > 3 and clean_word not in self.stop_words:\n                    cleaned_words.append(clean_word)\n            \n            # Count word frequencies\n            word_freq = Counter(cleaned_words)\n            \n            # Score keywords\n            keyword_scores = []\n            for word, freq in word_freq.items():\n                score = freq\n                \n                # Boost technical/academic terms\n                tech_terms = ['research', 'analys', 'study', 'method', 'algorithm', 'system', 'approach', 'model', 'data', 'information']\n                if any(term in word for term in tech_terms):\n                    score *= 2.0\n                \n                # Boost longer terms\n                if len(word) > 6:\n                    score *= 1.2\n                \n                keyword_scores.append({\n                    'word': word,\n                    'score': score,\n                    'frequency': freq\n                })\n            \n            return sorted(keyword_scores, key=lambda x: x['score'], reverse=True)[:max_keywords]\n            \n        except Exception as e:\n            print(f\"Keyword extraction error: {e}\")\n            return []\n    \n    def simple_summarize(self, text, max_sentences=3):\n        \"\"\"Create a simple extractive summary\"\"\"\n        if not text or len(text) < 200:\n            return text\n        \n        try:\n            # Simple sentence splitting\n            sentences = re.split(r'[.!?]+', text)\n            sentences = [s.strip() for s in sentences if len(s.strip()) > 30]\n            \n            if len(sentences) <= max_sentences:\n                return '. '.join(sentences) + '.'\n            \n            # Simple scoring based on sentence length and position\n            scored_sentences = []\n            for i, sentence in enumerate(sentences):\n                score = len(sentence.split())  # Word count as base score\n                \n                # Boost first and last sentences\n                if i == 0:\n                    score *= 1.5\n                elif i == len(sentences) - 1:\n                    score *= 1.2\n                \n                # Boost sentences with keywords\n                sentence_lower = sentence.lower()\n                keywords = ['research', 'analysis', 'study', 'conclusion', 'result', 'method', 'approach']\n                keyword_boost = sum(1 for keyword in keywords if keyword in sentence_lower)\n                score += keyword_boost * 0.5\n                \n                scored_sentences.append((score, sentence, i))\n            \n            # Select top sentences and maintain order\n            top_sentences = sorted(scored_sentences, key=lambda x: x[0], reverse=True)[:max_sentences]\n            top_sentences = sorted(top_sentences, key=lambda x: x[2])  # Restore original order\n            \n            summary = '. '.join([sent[1] for sent in top_sentences]) + '.'\n            return summary\n            \n        except Exception as e:\n            print(f\"Summarization error: {e}\")\n            # Fallback: return first portion\n            return text[:1000] + \"...\" if len(text) > 1000 else text\n\n# Create global instance\npdf_extractor = GlobalPDFExtractor()\n\ndef process_pdf_global(pdf_bytes):\n    \"\"\"Main global function to process PDF bytes and return comprehensive results\"\"\"\n    try:\n        print(\"üêç Starting PDF processing with Python...\")\n        print(f\"üìä Received {len(pdf_bytes)} bytes\")\n        \n        # Convert JavaScript Uint8Array to Python bytes properly\n        if hasattr(pdf_bytes, 'to_py'):\n            # Pyodide typed array - convert to Python\n            pdf_bytes_converted = pdf_bytes.to_py()\n            print(f\"üìã Converted to Python: {type(pdf_bytes_converted)}\")\n        else:\n            # Already Python-compatible\n            pdf_bytes_converted = pdf_bytes\n            print(f\"üìã Using as-is: {type(pdf_bytes_converted)}\")\n        \n        # Convert memoryview to bytes if needed\n        if isinstance(pdf_bytes_converted, memoryview):\n            pdf_bytes_py = bytes(pdf_bytes_converted)\n            print(f\"üìã Converted memoryview to bytes: {type(pdf_bytes_py)}\")\n        else:\n            pdf_bytes_py = pdf_bytes_converted\n        \n        # Now we can safely slice\n        first_20 = list(pdf_bytes_py[:20]) if len(pdf_bytes_py) >= 20 else list(pdf_bytes_py)\n        print(f\"üìã First 20 bytes: {first_20}\")\n        \n        # Check if this looks like a PDF\n        pdf_header = pdf_bytes_py[:4] if len(pdf_bytes_py) >= 4 else pdf_bytes_py\n        print(f\"üìã PDF header: {pdf_header}\")\n        print(f\"üìã PDF header type: {type(pdf_header)}\")\n        \n        if not pdf_header.startswith(b'%PDF'):\n            print(\"‚ùå Not a valid PDF - missing PDF header\")\n            return json.dumps({\n                'success': False,\n                'error': 'Invalid PDF file - missing PDF header',\n                'text': '',\n                'summary': '',\n                'keywords': [],\n                'processing_method': 'python_pypdf2'\n            })\n        \n        print(\"‚úÖ Valid PDF header detected!\")\n        \n        # Extract text from PDF using the converted bytes\n        extraction_result = pdf_extractor.extract_text_from_pdf_bytes(pdf_bytes_py)\n        print(f\"üìã Extraction result: {extraction_result}\")\n        \n        if not extraction_result['success']:\n            print(f\"‚ùå PDF extraction failed: {extraction_result['error']}\")\n            return json.dumps(extraction_result)\n        \n        text = extraction_result['text']\n        print(f\"üìù Extracted text length: {len(text)}\")\n        print(f\"üìù First 100 chars: {text[:100]}\")\n        \n        # Generate summary\n        summary = pdf_extractor.simple_summarize(text, 3)\n        print(f\"üìÑ Summary length: {len(summary)}\")\n        \n        # Extract keywords\n        keywords = pdf_extractor.extract_keywords(text, 8)\n        print(f\"üîë Extracted {len(keywords)} keywords\")\n        \n        result = {\n            'success': True,\n            'text': text,\n            'summary': summary,\n            'keywords': keywords,\n            'char_count': extraction_result['char_count'],\n            'word_count': extraction_result['word_count'],\n            'page_count': extraction_result['page_count'],\n            'processing_method': 'python_pypdf2'\n        }\n        \n        print(\"‚úÖ PDF processing completed successfully\")\n        return json.dumps(result)\n        \n    except Exception as e:\n        print(f\"‚ùå PDF processing failed with exception: {e}\")\n        print(f\"‚ùå Exception type: {type(e)}\")\n        import traceback\n        print(f\"‚ùå Traceback: {traceback.format_exc()}\")\n        \n        error_result = {\n            'success': False,\n            'error': str(e),\n            'text': '',\n            'summary': '',\n            'keywords': [],\n            'processing_method': 'python_pypdf2'\n        }\n        return json.dumps(error_result)\n\ndef summarize_text_global(text, max_sentences=3):\n    \"\"\"Global function for text summarization\"\"\"\n    try:\n        return pdf_extractor.simple_summarize(text, max_sentences)\n    except Exception as e:\n        print(f\"Summarization error: {e}\")\n        return text[:1000] + \"...\" if len(text) > 1000 else text\n\ndef extract_keywords_global(text, max_keywords=8):\n    \"\"\"Global function for keyword extraction\"\"\"\n    try:\n        keywords = pdf_extractor.extract_keywords(text, max_keywords)\n        return json.dumps(keywords)\n    except Exception as e:\n        print(f\"Keyword extraction error: {e}\")\n        return json.dumps([])\n\n# Expose functions to JavaScript\njs.processPDFGlobal = process_pdf_global\njs.summarizeTextGlobal = summarize_text_global\njs.extractKeywordsGlobal = extract_keywords_global\n\nprint(\"üéâ Global PDF Processor initialized successfully!\")\nprint(\"Available functions:\")\nprint(\"  - processPDFGlobal(pdf_bytes)\")\nprint(\"  - summarizeTextGlobal(text, max_sentences)\")\nprint(\"  - extractKeywordsGlobal(text, max_keywords)\")\n            ";await window.pdfProcessor.pyodide.runPython(e)}window.pdfProcessor.ready=!0,window.pdfProcessor.loading=!1,window.pdfProcessor.pdfSupport=e,e?console.log("‚úÖ Global PDF Processor is ready with full PDF support!"):console.log("‚úÖ Global PDF Processor is ready with text processing only!"),window.dispatchEvent(new CustomEvent("globalPDFProcessorReady",{detail:{ready:!0,timestamp:Date.now()}}))}catch(e){console.error("‚ùå Failed to initialize Global PDF Processor:",e),window.pdfProcessor.ready=!1,window.pdfProcessor.loading=!1,window.pdfProcessor.error=e.message,window.dispatchEvent(new CustomEvent("globalPDFProcessorError",{detail:{error:e.message,timestamp:Date.now()}}))}}window.pdfProcessor={ready:!1,loading:!1,error:null,pyodide:null,pdfSupport:!1},window.processFileWithPython=async function(e){if(!window.pdfProcessor.ready)throw new Error("Global PDF Processor not ready");try{const n=new Uint8Array(e),t=window.pdfProcessor.pyodide.globals.get("process_pdf_global");if(!t)throw new Error("process_pdf_global function not found in Python globals");const r=t(n);return JSON.parse(r)}catch(e){throw console.error("Error in processFileWithPython:",e),e}},window.summarizeTextWithPython=async function(e,n=3){if(!window.pdfProcessor.ready)throw new Error("Global PDF Processor not ready");try{const t=window.pdfProcessor.pyodide.globals.get("summarize_text_global");if(!t)throw new Error("summarize_text_global function not found in Python globals");return t(e,n)}catch(e){throw console.error("Error in summarizeTextWithPython:",e),e}},window.extractKeywordsWithPython=async function(e,n=8){if(!window.pdfProcessor.ready)throw new Error("Global PDF Processor not ready");try{const t=window.pdfProcessor.pyodide.globals.get("extract_keywords_global");if(!t)throw new Error("extract_keywords_global function not found in Python globals");const r=t(e,n);return JSON.parse(r)}catch(e){throw console.error("Error in extractKeywordsWithPython:",e),e}},window.getPyodideStatus=function(){return{ready:window.pdfProcessor.ready,loading:window.pdfProcessor.loading,error:window.pdfProcessor.error,pdfSupport:window.pdfProcessor.pdfSupport||!1}},window.isPyodideReady=function(){return!0===window.pdfProcessor.ready},document.addEventListener("DOMContentLoaded",(function(){console.log("üöÄ Starting Global PDF Processor initialization..."),setTimeout((()=>{initGlobalPDFProcessor()}),500)})),"loading"===document.readyState||(console.log("üöÄ DOM already loaded, starting Global PDF Processor..."),setTimeout((()=>{initGlobalPDFProcessor()}),500))</script><script defer="defer" src="/document-clustering-app/static/js/main.7a068596.js"></script><link href="/document-clustering-app/static/css/main.e6c13ad2.css" rel="stylesheet"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div><div id="pdf-processor-status" style="position:fixed;bottom:10px;right:10px;padding:8px 12px;background:#f3f4f6;border-radius:6px;font-size:12px;font-family:monospace;z-index:9999;display:none">PDF Processor: <span id="status-text">Loading...</span></div><script>function updateStatusIndicator(){const t=document.getElementById("pdf-processor-status"),e=document.getElementById("status-text");if(t&&e){const o=window.getPyodideStatus();o.loading?(e.textContent="Loading...",t.style.background="#fef3c7",t.style.color="#92400e"):o.ready?(o.pdfSupport?e.textContent="Ready üêçüìÑ":e.textContent="Ready üêç‚ö°",t.style.background="#dcfce7",t.style.color="#166534",setTimeout((()=>{t.style.display="none"}),3e3)):o.error&&(e.textContent="Error ‚ùå",t.style.background="#fee2e2",t.style.color="#dc2626"),t.style.display="block"}}window.addEventListener("globalPDFProcessorReady",updateStatusIndicator),window.addEventListener("globalPDFProcessorError",updateStatusIndicator);const statusCheck=setInterval((()=>{const t=window.getPyodideStatus();updateStatusIndicator(),(t.ready||t.error)&&clearInterval(statusCheck)}),1e3);document.addEventListener("DOMContentLoaded",(()=>{updateStatusIndicator()}))</script></body></html>